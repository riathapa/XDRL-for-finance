{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "from argparse import ArgumentParser\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "from decimal import Decimal\n",
    "import matplotlib.pyplot as plt\n",
    "import ornstein_uhlenbeck\n",
    "import csv\n",
    "import os\n",
    "\n",
    "import environment\n",
    "\n",
    "eps=10e-8\n",
    "epochs=0\n",
    "M=0\n",
    "\n",
    "# Create the necessary directories\n",
    "os.makedirs('../saved_network/PPO', exist_ok=True)\n",
    "\n",
    "class StockTrader():\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.wealth = 10e3\n",
    "        self.total_reward = 0\n",
    "        self.ep_ave_max_q = 0\n",
    "        self.loss = 0\n",
    "        self.actor_loss=0\n",
    "\n",
    "        self.wealth_history = []\n",
    "        self.r_history = []\n",
    "        self.w_history = []\n",
    "        self.p_history = []\n",
    "\n",
    "        self.noise = ornstein_uhlenbeck.OrnsteinUhlenbeckActionNoise(mu=np.zeros(M))\n",
    "\n",
    "    def update_summary(self,loss,r,q_value,actor_loss,w,p):\n",
    "        self.loss += loss\n",
    "        self.actor_loss+=actor_loss\n",
    "        self.total_reward+=r\n",
    "        self.ep_ave_max_q += q_value\n",
    "        self.r_history.append(r)\n",
    "        self.wealth = self.wealth * math.exp(r)\n",
    "        self.wealth_history.append(self.wealth)\n",
    "        self.w_history.extend([','.join([str(Decimal(str(w0)).quantize(Decimal('0.00'))) for w0 in w.tolist()[0]])])\n",
    "        self.p_history.extend([','.join([str(Decimal(str(p0)).quantize(Decimal('0.000'))) for p0 in p.tolist()])])\n",
    "\n",
    "    def write(self,epoch):\n",
    "        wealth_history = pd.Series(self.wealth_history)\n",
    "        r_history = pd.Series(self.r_history)\n",
    "        w_history = pd.Series(self.w_history)\n",
    "        p_history = pd.Series(self.p_history)\n",
    "        history = pd.concat([wealth_history, r_history, w_history, p_history], axis=1)\n",
    "        history.columns = ['Wealth', 'Return', 'Weight', 'Price']\n",
    "        history.to_csv('result' + str(epoch) + '-' + str(math.exp(np.sum(self.r_history)) * 100) + '.csv')\n",
    "\n",
    "    def print_result(self,epoch,agent):\n",
    "        self.total_reward=math.exp(self.total_reward) * 100\n",
    "        print('*-----Episode: {:d}, Reward:{:.6f}%,  ep_ave_max_q:{:.2f}, actor_loss:{:2f}-----*'.format(epoch, self.total_reward,self.ep_ave_max_q,self.actor_loss))\n",
    "        agent.write_summary(self.loss, self.total_reward,self.ep_ave_max_q,self.actor_loss, epoch)\n",
    "        agent.save_model(epoch)\n",
    "\n",
    "    def plot_result(self):\n",
    "        pd.Series(self.wealth_history).plot()\n",
    "        plt.show()\n",
    "\n",
    "    def action_processor(self,a,ratio):\n",
    "        a = np.clip(a + self.noise() * ratio, 0, 1)\n",
    "        a = a / (a.sum() + eps)\n",
    "        return a\n",
    "\n",
    "def parse_info(info):\n",
    "    return info['reward'],info['continue'],info[ 'next state'],info['weight vector'],info ['price'],info['risk']\n",
    "\n",
    "def save_state_actions(filename, states, actions):\n",
    "    \"\"\"\n",
    "    Save the states and actions to a csv file\n",
    "    \"\"\"\n",
    "    # Clean the array strings\n",
    "    states = [str(state).replace('\\n', '').replace('   ', ' ').replace('  ', ' ').replace('. ', '.0') for state in states]\n",
    "    actions = [str(action).replace('\\n', '').replace('  ', ' ') for action in actions]\n",
    "\n",
    "    with open(filename, mode='w', newline='') as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow(['State', 'Action'])\n",
    "        for state, action in zip(states, actions):\n",
    "            writer.writerow([state, action])\n",
    "\n",
    "def traversal(stocktrader,agent,env,epoch,noise_flag,framework,method,trainable):\n",
    "    info = env.step(None,None)\n",
    "    r,contin,s,w1,p,risk=parse_info(info)\n",
    "    contin=1\n",
    "    states = []\n",
    "    actions = []\n",
    "    while contin:\n",
    "        w2 = agent.predict(s)\n",
    "        # print(s.shape,w2.shape)\n",
    "\n",
    "        states.append(s)\n",
    "        actions.append(w2)\n",
    "\n",
    "        if noise_flag=='True':\n",
    "            w2=stocktrader.action_processor(w2,(epochs-epoch)/epochs)\n",
    "\n",
    "        env_info = env.step(w1, w2)\n",
    "        r, contin, s_next, w1, p,risk = parse_info(env_info)\n",
    "\n",
    "        agent.save_transition(s, w2, r-risk, contin, s_next, w1)\n",
    "        loss, q_value,actor_loss=0,0,0\n",
    "\n",
    "        if framework=='DDPG':\n",
    "            if trainable==\"True\":\n",
    "                agent_info= agent.train(method,epoch)\n",
    "                loss, q_value=agent_info[\"critic_loss\"],agent_info[\"q_value\"]\n",
    "                if method=='model_based':\n",
    "                    actor_loss=agent_info[\"actor_loss\"]\n",
    "\n",
    "        elif framework=='PPO':\n",
    "            if not contin and trainable==\"True\":\n",
    "                agent_info = agent.train(method, epoch)\n",
    "                loss, q_value = agent_info[\"critic_loss\"], agent_info[\"q_value\"]\n",
    "                if method=='model_based':\n",
    "                    actor_loss=agent_info[\"actor_loss\"]\n",
    "\n",
    "        stocktrader.update_summary(loss,r,q_value,actor_loss,w2,p)\n",
    "        s = s_next\n",
    "    save_state_actions(f\"state_action_recopilation.csv\", states, actions)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def parse_config(config,mode):\n",
    "    codes = config[\"session\"][\"codes\"]\n",
    "    start_date = config[\"session\"][\"start_date\"]\n",
    "    end_date = config[\"session\"][\"end_date\"]\n",
    "    features = config[\"session\"][\"features\"]\n",
    "    agent_config = config[\"session\"][\"agents\"]\n",
    "    market = config[\"session\"][\"market_types\"]\n",
    "    noise_flag, record_flag, plot_flag=config[\"session\"][\"noise_flag\"],config[\"session\"][\"record_flag\"],config[\"session\"][\"plot_flag\"]\n",
    "    predictor, framework, window_length = agent_config\n",
    "    reload_flag, trainable=config[\"session\"]['reload_flag'],config[\"session\"]['trainable']\n",
    "    method=config[\"session\"]['method']\n",
    "\n",
    "    global epochs\n",
    "    epochs = int(config[\"session\"][\"epochs\"])\n",
    "\n",
    "    if mode=='test':\n",
    "        record_flag='True'\n",
    "        noise_flag='False'\n",
    "        plot_flag='True'\n",
    "        reload_flag='True'\n",
    "        trainable='False'\n",
    "        method='model_free'\n",
    "\n",
    "    print(\"*--------------------Training Status-------------------*\")\n",
    "    print('Codes:',codes)\n",
    "    print(\"Date from\",start_date,' to ',end_date)\n",
    "    print('Features:',features)\n",
    "    print(\"Agent:Noise(\",noise_flag,')---Recoed(',noise_flag,')---Plot(',plot_flag,')')\n",
    "    print(\"Market Type:\",market)\n",
    "    print(\"Predictor:\",predictor,\"  Framework:\", framework,\"  Window_length:\",window_length)\n",
    "    print(\"Epochs:\",epochs)\n",
    "    print(\"Trainable:\",trainable)\n",
    "    print(\"Reloaded Model:\",reload_flag)\n",
    "    print(\"Method\",method)\n",
    "    print(\"Noise_flag\",noise_flag)\n",
    "    print(\"Record_flag\",record_flag)\n",
    "    print(\"Plot_flag\",plot_flag)\n",
    "\n",
    "\n",
    "    return codes,start_date,end_date,features,agent_config,market,predictor, framework, window_length,noise_flag, record_flag, plot_flag,reload_flag,trainable,method\n",
    "\n",
    "def session(config,mode):\n",
    "    import environment\n",
    "    codes, start_date, end_date, features, agent_config, market,predictor, framework, window_length,noise_flag, record_flag, plot_flag,reload_flag,trainable,method=parse_config(config,mode)\n",
    "    print(\"Market : \", market)\n",
    "    env = environment.Environment(start_date, end_date, codes, features, int(window_length), market)\n",
    "\n",
    "\n",
    "    global M\n",
    "    M=len(codes)+1\n",
    "\n",
    "    if framework == 'DDPG':\n",
    "        print(\"*-----------------Loading DDPG Agent---------------------*\")\n",
    "        from ddpg import DDPG\n",
    "        agent = DDPG(predictor, len(codes) + 1, int(window_length), len(features), '-'.join(agent_config), reload_flag,trainable)\n",
    "\n",
    "    elif framework == 'PPO':\n",
    "        print(\"*-----------------Loading PPO Agent---------------------*\")\n",
    "        import PPO\n",
    "        agent = PPO(predictor, len(codes) + 1, int(window_length), len(features), '-'.join(agent_config), reload_flag,trainable)\n",
    "\n",
    "    stocktrader=StockTrader()\n",
    "\n",
    "    if mode=='train':\n",
    "\n",
    "        print(\"Training with {:d}\".format(epochs))\n",
    "        for epoch in range(epochs):\n",
    "            print(\"Now we are at epoch\", epoch)\n",
    "            traversal(stocktrader,agent,env,epoch,noise_flag,framework,method,trainable)\n",
    "\n",
    "            if record_flag=='True':\n",
    "                stocktrader.write(epoch)\n",
    "\n",
    "            if plot_flag=='True':\n",
    "                stocktrader.plot_result()\n",
    "\n",
    "            stocktrader.print_result(epoch,agent)\n",
    "            stocktrader.reset()\n",
    "\n",
    "    elif mode=='test':\n",
    "        traversal(stocktrader, agent, env, 1, noise_flag,framework,method,trainable)\n",
    "        stocktrader.write(1)\n",
    "        stocktrader.plot_result()\n",
    "        stocktrader.print_result(1, agent)\n",
    "\n",
    "def build_parser():\n",
    "    parser = ArgumentParser(description='Provide arguments for training different DDPG or PPO models in Portfolio Management')\n",
    "    parser.add_argument(\"--mode\",dest=\"mode\",help=\"download(China), train, test\",metavar=\"MODE\", default=\"train\",required=True)\n",
    "    parser.add_argument(\"--model\",dest=\"model\",help=\"DDPG,PPO\",metavar=\"MODEL\", default=\"DDPG\",required=False)\n",
    "    return parser\n",
    "\n",
    "\n",
    "def main():\n",
    "    parser = build_parser()\n",
    "    args=vars(parser.parse_args())\n",
    "    print(args)\n",
    "    with open('../config.json') as f:\n",
    "        config=json.load(f)\n",
    "        if args['mode']=='download':\n",
    "            from data.download_data import DataDownloader\n",
    "            data_downloader=DataDownloader(config)\n",
    "            data_downloader.save_data()\n",
    "        else:\n",
    "            session(config,args['mode'])\n",
    "\n",
    "if __name__==\"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
